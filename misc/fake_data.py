from zgulde.ds_imports import *
from zgulde.ds_util import rand_dates

##############################################################################

#: ## Fake Time Series Data

def fake_seasonality_data(
    n=1000,
    scale=1,
    center=0,
    cycles=2,
    date_range=None,
    trend="positive",
    add_noise=True,
):
    seasonal = scale * np.sin(np.linspace(-cycles * np.pi, cycles * np.pi, n)) + center
    trend = np.linspace(0, scale * (1 if trend is "positive" else -1), n)
    y = seasonal + trend
    if add_noise:
        noise = np.random.normal(0, scale * 0.6, n)
        y = y + noise
    if date_range is not None:
        return pd.Series(y, index=date_range)
    return y


y = fake_seasonality_data(
    n=1000, scale=4, center=0, cycles=8, trend="positive", add_noise=False
)
plt.plot(y)

pd.Series(y, index=pd.date_range("20180101", freq="D", periods=N))

x = rand_dates("2018-01-01 00:00:00", "2018-01-01 23:59:59", 1000)
pd.Series(y, index=x.sort_values()).plot()

rand_dates('2019', '2020', 365).sort_values()

##############################################################################

#: ## Creating Categorical Features With sklearn's Data Generation
#:
#: TODO
#:
#: - add missing values
#: - introduce typos in categoricals
#: - add outliers
#: - skewed / not normal distributions for numeric vars (stats.skewnorm?
#:   quantile transform?)

import random as r
import numpy as np
import pandas as pd
from sklearn import datasets
from string import ascii_letters as letters
from cool_plots import scatter_by_group
import matplotlib.pyplot as plt
import seaborn as sns
import zgulde.ds_util.plotting as zplot
plt.style.use(zplot.style)

colors = ['red', 'orange', 'yellow', 'green', 'blue', 'indigo', 'violet']
def random_colors(n):
    return np.random.choice(colors, n, replace=False)

fruits = ['kiwi', 'strawberry', 'banana', 'apple', 'peach', 'pear', 'blueberry']
def random_fruits(n):
    return np.random.choice(fruits, n, replace=False)

def transform_categorical(col):
    n = r.randint(2, 7)
    f = r.choice([random_colors, random_fruits])
    labels = f(n)
    cutting_fn = r.choice([pd.cut, pd.qcut])
    return cutting_fn(col, n, labels=labels)

def transform_numerical(col):
    '''assumes col is already standard normal'''
    mean, sd = r.choice([
        (100, 10),
        (100, 50),
        (1000, 10),
        (1000, 100),
        (1000, 300),
        (0, 4),
        (0, 1),
        (8, 1),
        (12, 2),
        (15, 3)
    ])
    col = mean + sd * col
    return col.round().astype(int) if np.random.choice([True, False]) else col

def transform_col(col, p_categorical):
    f = np.random.choice(
        [transform_categorical, transform_numerical],
        p=[p_categorical, 1 - p_categorical],
    )
    return f(col)

def transform_cols(df, p_categorical=.5, inplace=False):
    """
    Takes in a dataframe generated by sklearn's make_* and randomly turns each
    column into a new continuous or categorical feature.

    Note that p_categorical is not gauranteed, buy rather a probability used in
    the random choice of a column transformation.
    """
    if not inplace:
        df = df.copy()
    for col in df:
        df[col] = transform_col(df[col], p_categorical)
    return df

# seed = 43
# np.random.seed(seed)
# r.seed(seed)
X, y, coefs = datasets.make_regression(
    n_samples=500,
    n_features=5,
    n_informative=3,
    n_targets=1,
    coef=True,
)
df = (
    pd.DataFrame(X)
    .set_axis(list(letters[:X.shape[1]]), axis=1)
    .pipe(transform_cols, p_categorical=1)
    .assign(y=y)
)

##############################################################################

#: ## Other Fake Data Sources

#: -------------------------------------

#: https://faker.readthedocs.io/en/master/

from faker import Faker

faker = Faker()

df = pd.DataFrame([faker.profile() for _ in range(1000)])
df

df.head(1).T

df.company.value_counts()

df.job.value_counts()

#: ------------------

#: datafairy by basecamp generates realistic fake data

#: ------------------

faker = Faker()

def make_typo(s):
    if s == 'male':
        return np.random.choice(['MALE', 'M', 'm', 'ale'])
    else:
        return np.random.choice(['FEMALE', 'F', 'f', 'emale'])
def random_sex(n=100, p_typos=.6):
    x = pd.Series(np.random.choice(['male', 'female'], n))
    x = x.where(np.random.rand(n) > p_typos, x.apply(make_typo))
    return x

Faker.seed(123)
faker = Faker()
np.random.seed(123)
n = 1000
applicants = pd.DataFrame([faker.profile() for _ in range(100)])
applicants[[]]
applicants.columns


def make_typo(s):
    if s == 'male':
        return np.random.choice(['MALE', 'M', 'm', 'ale'])
    else:
        return np.random.choice(['FEMALE', 'F', 'f', 'emale'])
def random_sex(n=100, p_typos=.6):
    x = pd.Series(np.random.choice(['male', 'female'], n))
    x = x.where(np.random.rand(n) > p_typos, x.apply(make_typo))
    return x

#: ## Ideas for fake data
#:
#: codeup db tables
#:
#: - applicants: user_id, name, sex, address, industry, birthdate, etc
#: - contacts: user_id, time series data -- visits to codeup.com? phone calls (type, duration)?
#: - students: user_id, student_id, admitted_at, cohort
#: - grades: user_id, module_id, grade
#:
#: ideas
#:
#: - contacts table for time series, anomaly detection
#: - not all applicants have student records, use this to calculate conversion
#:   rate
#: - applicants from one industry tend to have higher grades
#: - older students tend to do
#: - variations in numerical by cohort
#: - aggregated measures from contacts correlate with grades
#: - intentional typos in field values and field names

#: ----

#: ## Fake or Example Databases

#: https://www.analyticsvidhya.com/blog/2018/05/24-ultimate-data-science-projects-to-boost-your-knowledge-and-skills/

#: This is good!
#:
#: https://www.mysqltutorial.org/mysql-sample-database.aspx
#: https://www.sqlitetutorial.net/sqlite-sample-database/
#: https://wiki.postgresql.org/wiki/Sample_Databases

#: ---

#: faker.time_series() looks promising
